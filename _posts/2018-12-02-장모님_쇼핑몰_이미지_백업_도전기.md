---
title: 장모님 쇼핑몰 이미지 백업 도전기
tags: [python, scrapy, crawling, image, 파이썬, 스크래피, 크롤링, 이미지]
---

## 쇼핑몰이 왜 이러지

저희 장모님께서는 오래전부터 쇼핑몰을 만들어주는 정부지원사업(제작부터 운영까지 정부에서 지정한 업체가 대행)을 통해 화분 도자기 쇼핑몰 운영해오셨습니다. 그런데 얼마전부터 웹사이트 접속이 일시적으로 안되며 서버가 불안정한 모습을 보였다고 하셨습니다.

장모님께서 담당 부서에 연락해보니 정부지원이 끝나서 웹사이트의 호스팅이 종료될 예정이라는 통보를 받으셨습니다. 말하는 바를 모두 이해할 수 없었지만, 그 과정에서 관리부실이든 인프라축소든 어떤 이유로 인해 서버가 불안정해진 것 같다고 말씀하셨죠.

그래서 장모님은 카페24로 이전을 결정하셨습니다. 회원정보는 관리페이지에서 엑셀파일로 백업. 그러나 이미 업로드한 만개가 넘는 상품 이미지들은 일괄 다운로드 기능이 없어서 틈틈이 수동으로 저장하고 계셨습니다.

이 이야기를 우연히 듣게된 저는 도와드리기로 마음먹었습니다.

## 이미지 크롤링을 해보자

도구는 파이썬 기반의 사용해본 경험이 있는 [`Scrapy`](https://scrapy.org/)로 결정했습니다.

### 크롤링 대상 URI 분석 및 요청 코드 작성

먼저 **크롤링할 페이지의 URI** 와 **페이지에서 추출할 데이터의 Path**(xpath 형식이든 css 형식이든) 두 가지 정보가 필요합니다.

제가 크롤링할 상품별 이미지들은 각 상품들의 상세페이지에 있었으며, 그 페이지의 URI 는 다음과 같았습니다. *(사생활 보호를 위해 실제 도메인이 아닌 가상의 도메인을 넣었습니다.)*

`http://www.jangmodojagi.co.kr/goods/content.asp?guid=1234`

맨 마지막의 `guid` 값만 원하는 상품의 guid로 바꿔주면 해당 상품의 상세페이지에 접근이 가능했습니다. 상품들의 guid 리스트는 관리 페이지 기능을 이용해 파일로 저장했으며, 이를 읽어와 페이지에 접근할 수 있도록 코드를 작성했습니다.

```python
main_url = 'http://www.jangmodojagi.co.kr'
goods_info_uri = '/goods/content.asp?guid='

def start_requests(self):
    guids = open('guids', 'r')

    urls = []
    for guid in guids:
        urls.append(self.main_url + self.goods_info_uri + guid.strip())

    for url in urls:
        yield scrapy.Request(url=url, callback=self.parse)
```

### 페이지 분석 및 이미지 URI 데이터 추출 코드 작성

페이지에서 원하는 데이터만 추출하기 위해서는 패스가 필요합니다. 저는 개인적으로 익숙한 xpath 형식의 패스를 사용했습니다.

패스를 찾을 때는 코드를 수정하고 실행하는 것보다 `scrapy shell` 명령이 더 간단합니다.

```bash
$ scrapy shell "http://www.jangmodojagi.co.kr/goods/content.asp?guid=1234"
2018-12-02 20:44:25 [scrapy.utils.log] INFO: Scrapy 1.5.1 started (bot: image_crawler)
2018-12-02 20:44:25 [scrapy.utils.log] INFO: Versions: lxml 4.2.5.0, libxml2 2.9.4, cssselect 1.0.3, parsel 1.5.0, w3lib 1.19.0, Twisted 18.7.0, Python 3.4.2 (default, Sep 12 2018, 11:34:10) - [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)], pyOpenSSL 18.0.0 (OpenSSL 1.1.0i  14 Aug 2018), cryptography 2.3.1, Platform Darwin-18.2.0-x86_64-i386-64bit

...
중략
...

2018-12-02 20:44:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.jangmodojagi.co.kr/main/main_real.asp> (referer: None) ['partial']
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x10b7c8da0>
[s]   item       {}
[s]   request    <GET http://www.jangmodojagi.co.kr>
[s]   response   <200 http://www.jangmodojagi.co.kr/main/main_real.asp>
[s]   settings   <scrapy.settings.Settings object at 0x10b7c8e48>
[s]   spider     <DefaultSpider 'default' at 0x10d25f048>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser
>>> response.xpath("//any/tag/of/path").extract()
```

위와 같이 페이지는 한 번만 요청한 상태에서 반복적으로 패스를 변경하며 확인할 수 있어서 훨씬 효과적으로 실험 할 수 있습니다. 이를 통해 상품 이미지들의 URI 를 추출하는 코드를 작성했습니다.

```python
def parse(self, response):
    hxs = Selector(response)

    guid = response.url.split('=')[1];
    image_uris = hxs.xpath("//table[@id='tbContent']//img/@src").extract()

    print(guid, '\n', image_uris)
```

### 이미지 파일 저장

추출된 URI 는 절대경로가 아닌 상대경로 방식이어서 도메인과 결합해 사용했으며, 이미지를 가져와 저장하는 라이브러리는 `urllib` 를 사용했습니다.

```python
for image_uri in image_uris:
    image_name = guid + '_' + image_uri.split('/')[-1]
    image_url = self.main_url + image_uri

    print(image_name, image_url)
    urllib.request.urlretrieve(image_url, '/Users/copyx/PycharmProjects/image_crawler/image_crawler/images/'
                                + image_name)
```

## 장모님을 위한 크롤러 완성

`scrapy crawl` 명령을 이용해 스파이더를 실행해보니 잘 돌아갑니다. :)

추가적으로, 크롤링 속도를 적당히 느리게 하기 위해 `settings.py` 파일에서 몇 가지 옵션을 변경했습니다.

```python
# CONCURRENT_REQUESTS = 5 # 주석처리함
DOWNLOAD_DELAY = 1 # 다운로드 간격 설정
```

## 더 생각해볼 부분

- `urllib.request.urlretrieve` 는 왜 절대 경로만 통하는 것일까?
- `pipelines.py`, `middlewares.py` 파일의 자세한 기능
- 글쓰다 알게된 다른 방법 [Scrapy Images Pipeline](https://doc.scrapy.org/en/latest/topics/media-pipeline.html#using-the-images-pipeline)
  - 굳이 urllib 안쓰고 스크래피의 item 과 pipeline 사용하는 방법이 있음...~~삽질했다...~~

## 참고

- [Scrapy Tutorial - Scrapy 1.5.1 documentation](https://docs.scrapy.org/en/latest/intro/tutorial.html)
- [python - Scrapy is not able to download the images from a URL - Stack Overflow](https://stackoverflow.com/questions/52566417/scrapy-is-not-able-to-download-the-images-from-a-url)
